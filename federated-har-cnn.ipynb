{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "# Tensorflow Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Asynchronous, needed for federated learning \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Others\n",
    "import collections\n",
    "import attr\n",
    "import functools\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Human activity recognition (HAR) using convolutional neural network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code reference : [CNN models for human activity recognition time-series classification](https://machinelearningmastery.com/cnn-models-for-human-activity-recognition-time-series-classification/)\n",
    "\n",
    "CNN algorithm is selected based on [this blog's information](https://machinelearningmastery.com/deep-learning-models-for-human-activity-recognition/), quoted:\n",
    "\n",
    "    \"When applied to time series classification like HAR, CNN has two advantages over other models: local dependency and scale invariance. Local dependency means the nearby signals in HAR are likely to be correlated, while scale invariance refers to the scale-invariant for different paces or frequencies.\"\n",
    "   ([Original source](https://arxiv.org/pdf/1707.03502.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from a client, returns train dataset (X) and label (y) elements\n",
    "def load_dataset(subject_num=1):\n",
    "    # import csv\n",
    "    df = pd.read_csv('dataset/mHealth_subject' + str(subject_num+1) + '.csv', header=None)\n",
    "    \n",
    "    # exclude 0\n",
    "    df = df[df[21] != 0]\n",
    "    \n",
    "    # split to dataset and elements\n",
    "    trainy = df[21]\n",
    "    df_X = df.drop([21], axis=1)\n",
    "    \n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = list()\n",
    "    loaded.append(df_X.values)\n",
    "    trainX = np.dstack(loaded)\n",
    "    \n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "                            \n",
    "    return trainX, trainy\n",
    "\n",
    "# Fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy):\n",
    "    # set epochs and batch_size to 1 each due to its purpose solely as example and limiting resource\n",
    "    # set verbose to 1 to see training progress\n",
    "    verbose, epochs, batch_size = 1, 1, 1\n",
    "    \n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    \n",
    "    # Set precision and recall to calculate F1 score\n",
    "    precision = tf.keras.metrics.Precision(name='precision')\n",
    "    recall = tf.keras.metrics.Recall(name='recall')\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', precision, recall])\n",
    "    \n",
    "    # fit network\n",
    "    history = model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    # get evaluation metrics\n",
    "    accuracy = history.history['accuracy'][epochs-1]\n",
    "    precision = history.history['precision'][epochs-1]\n",
    "    recall = history.history['recall'][epochs-1]\n",
    "    \n",
    "    return accuracy, precision, recall\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores, f1):\n",
    "    m, s = np.mean(scores), np.std(scores)\n",
    "    m2, s2 = np.mean(f1), np.std(f1)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f), F1 score: %.3f%% (+/-%.3f)' % (m, s, m2, s2))\n",
    "\n",
    "# run an experiment\n",
    "def run_experiment(repeats=1):\n",
    "    # load data\n",
    "    trainX, trainy = load_dataset()\n",
    "    # repeat experiment\n",
    "    accuracies = list()\n",
    "    f1s = list()\n",
    "    for r in range(repeats):\n",
    "        accuracy, precision, recall = evaluate_model(trainX, trainy)\n",
    "        accuracy = accuracy * 100.0\n",
    "        f1_score = (2.0*((precision * recall)/(precision + recall))) * 100.0\n",
    "        print('> Iteration #%d: %.3f, F1: %.3f' % (r+1, accuracy, f1_score))\n",
    "        accuracies.append(accuracy)\n",
    "        f1s.append(f1_score)\n",
    "    # summarize results\n",
    "    summarize_results(accuracies, f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18432/18432 [==============================] - 23s 1ms/step - loss: 0.2503 - accuracy: 0.9191 - precision: 0.9263 - recall: 0.9126\n",
      "> Iteration #1: 91.905, F1: 91.941\n",
      "Accuracy: 91.905% (+/-0.000), F1 score: 91.941% (+/-0.000)\n"
     ]
    }
   ],
   "source": [
    "# run the experiment\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Federated learning for HAR using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code references :\n",
    "1. Previous section\n",
    "2. [Tensorflow federated learning homepage and API](https://www.tensorflow.org/federated)\n",
    "3. [Google Workshop on Federated Learning and Analytics](https://events.withgoogle.com/demostutorials-workshop-on-federated-learning-and-analytics-2020/)\n",
    "4. Our beloved [StackOverflow](https://stackoverflow.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to load_dataset() function, but returning tensorflow dataset in batch form\n",
    "def load_client_dataset(client_id=1):\n",
    "    # import csv\n",
    "    df = pd.read_csv('dataset/mHealth_subject' + str(client_id) + '.csv', header=None)\n",
    "    \n",
    "    # exclude 0\n",
    "    df = df[df[21] != 0]\n",
    "    \n",
    "    # split to features (X) and label (y)\n",
    "    y = df[21]\n",
    "    X = df.drop([21], axis=1)\n",
    "    X = np.expand_dims(X, axis=2)\n",
    "    \n",
    "    # one hot encode y\n",
    "    y = to_categorical(y)\n",
    "    \n",
    "    # create tensorflow dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X,y))\n",
    "    dataset = dataset.batch(20) # 20 rows for each batch\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create federated train data from 8 clients (not randomized for now)\n",
    "train_data = [load_client_dataset(x) for x in range(1,9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(21,1)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap a Keras model for use with TFF\n",
    "def model_fn():\n",
    "  keras_model = create_keras_model()\n",
    "  return tff.learning.from_keras_model(\n",
    "      keras_model,\n",
    "      input_spec=train_data[0].element_spec,\n",
    "      loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "      metrics=[tf.keras.metrics.Accuracy(name='acc'),\\\n",
    "               tf.keras.metrics.Precision(name='pr'),\\\n",
    "               tf.keras.metrics.Recall(name='rc')\\\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration #1 | Accuracy: 14.068%, F1 score: 95.258%\n",
      "> Iteration #2 | Accuracy: 14.500%, F1 score: 90.180%\n",
      "> Iteration #3 | Accuracy: 12.368%, F1 score: 89.535%\n",
      "> Iteration #4 | Accuracy: 12.281%, F1 score: 90.084%\n",
      "> Iteration #5 | Accuracy: 12.682%, F1 score: 91.171%\n",
      "> Iteration #6 | Accuracy: 12.332%, F1 score: 92.656%\n",
      "> Iteration #7 | Accuracy: 13.785%, F1 score: 93.487%\n",
      "> Iteration #8 | Accuracy: 14.820%, F1 score: 94.201%\n",
      "> Iteration #9 | Accuracy: 14.648%, F1 score: 94.470%\n",
      "> Iteration #10 | Accuracy: 14.259%, F1 score: 95.030%\n",
      "> Iteration #11 | Accuracy: 17.033%, F1 score: 95.171%\n",
      "> Iteration #12 | Accuracy: 16.523%, F1 score: 95.323%\n",
      "> Iteration #13 | Accuracy: 17.197%, F1 score: 95.694%\n",
      "> Iteration #14 | Accuracy: 20.688%, F1 score: 96.549%\n",
      "> Iteration #15 | Accuracy: 21.390%, F1 score: 96.623%\n",
      "> Iteration #16 | Accuracy: 24.991%, F1 score: 97.095%\n",
      "> Iteration #17 | Accuracy: 23.231%, F1 score: 96.726%\n",
      "> Iteration #18 | Accuracy: 25.803%, F1 score: 97.063%\n",
      "> Iteration #19 | Accuracy: 24.528%, F1 score: 96.898%\n",
      "> Iteration #20 | Accuracy: 24.144%, F1 score: 96.716%\n",
      "> Iteration #21 | Accuracy: 23.952%, F1 score: 96.838%\n",
      "> Iteration #22 | Accuracy: 25.893%, F1 score: 96.969%\n",
      "> Iteration #23 | Accuracy: 26.262%, F1 score: 97.353%\n",
      "> Iteration #24 | Accuracy: 26.518%, F1 score: 97.117%\n",
      "> Iteration #25 | Accuracy: 26.049%, F1 score: 96.727%\n"
     ]
    }
   ],
   "source": [
    "# Simulate federated learning with federated averaging as model aggregation\n",
    "trainer = tff.learning.build_federated_averaging_process(\n",
    "    model_fn,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(0.001))\n",
    "\n",
    "# Init\n",
    "state = trainer.initialize()\n",
    "\n",
    "# Simulate \"batch\" learning\n",
    "for i in range(25):\n",
    "    state, metrics = trainer.next(state, train_data)\n",
    "    print('> Iteration #%d | Accuracy: %.3f%%, F1 score: %.3f%%' % ( \\\n",
    "            i+1, \\\n",
    "            metrics['train']['acc'] * 100.0, \\\n",
    "            2.0*((metrics['train']['pr'] * metrics['train']['rc'])/(metrics['train']['pr'] + metrics['train']['rc'])) * 100.0 \\\n",
    "         )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
